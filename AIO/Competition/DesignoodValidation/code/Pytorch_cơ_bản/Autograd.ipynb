{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Autograd.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNOldNUulRK1zB+qhDTGCyn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JXq_kYaEWjgl"},"source":["Trong phần này sẽ tìm hiểu về cách tính đạo hàm bằng cách sử dụng autograd package trong Pytorch. "]},{"cell_type":"markdown","metadata":{"id":"Qaa1mZH8W9KV"},"source":["# 1- The Autograd package\n","Autograd package cung cấp một sự khác biệt tự động cho tất cả các hoạt động của tensor. Rất dễ dàng để sử dụng đạo hàm trong pytorch bằng cách chỉ cho nó biết rằng tensor cần được đạo hàm bằng *requires_grad*. Với việc thiết lập thuộc tính này, các phép toán trên tensor đều được theo dõi trên một đồ thị tính toán."]},{"cell_type":"code","metadata":{"id":"aBowUSb9VzLT","executionInfo":{"status":"ok","timestamp":1626356249964,"user_tz":-420,"elapsed":280,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}}},"source":["import torch\n","\n","x = torch.randn(3, requires_grad=True)\n","y = x + 2"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5i9QMvyuYLQ-","executionInfo":{"status":"ok","timestamp":1626356285346,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}},"outputId":"fb0ce8b8-8f56-4839-bdd9-230fe58c4845"},"source":["\"\"\"\n","  - ở cell trên y đã được tạo ra bởi kết quả của phép tính x + 2, vì vậy nó sẽ tạo một thuộc tính grad_fn.\n","  - grad_fn: tham chiếu đến một hàm đã tạo tensor\n","\"\"\"\n","print(x) # Đã được tạo ở cell tren -> grad_fn lúc này là None\n","print(y)\n","print(y.grad_fn)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["tensor([ 0.8886,  0.0518, -0.0723], requires_grad=True)\n","tensor([2.8886, 2.0518, 1.9277], grad_fn=<AddBackward0>)\n","<AddBackward0 object at 0x7f97a96e3c50>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4tX0nNWZElG","executionInfo":{"status":"ok","timestamp":1626356410628,"user_tz":-420,"elapsed":385,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}},"outputId":"ebd10f9f-67f1-4ce4-b38f-2a6eb4504442"},"source":["# Thực hiện các phép tính khác trên y\n","z = y * 3\n","print(z)\n","z =  z + 3\n","print(z)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["tensor([8.6658, 6.1553, 5.7831], grad_fn=<MulBackward0>)\n","tensor([11.6658,  9.1553,  8.7831], grad_fn=<AddBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lgVTTZIhZyGu"},"source":["# 2 - Tính đạo hàm với lan truyền ngược"]},{"cell_type":"markdown","metadata":{"id":"9zCN5ugaaL_G"},"source":["Khi hoàn tất quá trình tính toán, ta có thể gọi *.backward()* và tất cả giá trị đạo hàm sẽ được tính toán một cách tự động. Giá trị đạo hàm của những tensor này sẽ được tích lũy vào trong thuộc tính *.grad*. Nó chính là đọa hàm riêng của tensor."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JDIbCDjUZ1Km","executionInfo":{"status":"ok","timestamp":1626356905755,"user_tz":-420,"elapsed":3,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}},"outputId":"5a1cc6d6-7449-4053-d078-6516aa147e98"},"source":["z = z.mean()\n","z.backward()\n","print(x.grad) # dz/dx"],"execution_count":10,"outputs":[{"output_type":"stream","text":["tensor([1., 1., 1.])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SLsBYEbJbnNH","executionInfo":{"status":"ok","timestamp":1626357216079,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}},"outputId":"0c221d77-76db-4080-a727-aec8bfc38fc6"},"source":["x = torch.randn(5, requires_grad=True)\n","print(x)\n","y = x*2\n","for _ in range(10):\n","  y = y * 2\n","\n","print(y)\n","print(y.shape)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["tensor([-1.1477,  1.2997,  0.3202, -0.8102, -1.2584], requires_grad=True)\n","tensor([-2350.4673,  2661.7681,   655.8086, -1659.2811, -2577.2542],\n","       grad_fn=<MulBackward0>)\n","torch.Size([5])\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6GsvmmnucFuO","executionInfo":{"status":"ok","timestamp":1626357218060,"user_tz":-420,"elapsed":299,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}},"outputId":"5ea62e49-17e7-47f3-de7b-dcfa82711906"},"source":["w = torch.tensor([0.1, 1.0, 0.0001, 0.01, 0.001], dtype=torch.float32)\n","y.backward(w)\n","print(x.grad)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["tensor([2.0480e+02, 2.0480e+03, 2.0480e-01, 2.0480e+01, 2.0480e+00])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0GqR7LrqcxpP"},"source":["# 3 - Stop a tensor from tracking history"]},{"cell_type":"markdown","metadata":{"id":"6Bcd6V3Uc5K-"},"source":["Trong quá trình huấn luyện, khi chúng ta muốn cập nhật trọng số thì thao tác cập nhật này không nên là một phần của phép tính đạo hàm. Chúng ta có 3 sự lựa chọn cho việc dừng quá trình đạo hàm và cập nhật tham số như sau:\n","- x.requires_grad_false()\n","- x.detach()\n","- wrap in with torch.no_grad():"]},{"cell_type":"markdown","metadata":{"id":"ZR8X6xtffX54"},"source":["**.requires_grad_(...) thay đổi yêu cầu ngay tại vị trí cần yêu cầu đạo hàm**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X6bXhARNc4As","executionInfo":{"status":"ok","timestamp":1626357837496,"user_tz":-420,"elapsed":4,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}},"outputId":"ff143277-6795-419c-9c67-7858c110cfbd"},"source":["# requires_grad_()\n","a = torch.rand(2, 2)\n","print(a.requires_grad) # Kiểm tra a đã được yêu cầu tính đạo hàm hay chưa\n","b = (a*5) / (a-1)\n","print(b.grad_fn) # Do a chưa tính đạo hàm nên grad_fn lúc này sẽ là None \n","a.requires_grad_(True) # thiết lập tính đạo hàm cho a \n","print(a.requires_grad) \n","b = (a**2).sum()\n","print(b.grad_fn) # Sau khi thiết lập đạo hàm cho a. Thì phép tính b trên a sẽ nhận được grad_fn"],"execution_count":16,"outputs":[{"output_type":"stream","text":["False\n","None\n","True\n","<SumBackward0 object at 0x7f97a5b236d0>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Zb3TfvU3faaY"},"source":["**.detach(): Lấy một tensor mới với nội dung tương tự nhưng không yêu cầu tính đạo hàm**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FHlUpVB_feFW","executionInfo":{"status":"ok","timestamp":1626358149840,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}},"outputId":"977f8a21-be41-40e1-c184-e35c8967d436"},"source":["a = torch.rand(2, 2, requires_grad=True)\n","print(a.requires_grad)\n","print(a)\n","b = a.detach()\n","print(b.requires_grad)\n","print(b)"],"execution_count":18,"outputs":[{"output_type":"stream","text":["True\n","tensor([[0.2102, 0.7872],\n","        [0.6593, 0.0418]], requires_grad=True)\n","False\n","tensor([[0.2102, 0.7872],\n","        [0.6593, 0.0418]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nfVEkCbZgBA4"},"source":["**wrap in with torch.no_grad()**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"taDEBasWgDs_","executionInfo":{"status":"ok","timestamp":1626358250932,"user_tz":-420,"elapsed":3,"user":{"displayName":"Hung An Minh","photoUrl":"","userId":"05267040817390776748"}},"outputId":"06482288-cbbc-4bed-be73-e1f29ab29995"},"source":["a = torch.rand(2, 2, requires_grad=True)\n","print(a.requires_grad)\n","with torch.no_grad():\n","  print((x + 2).requires_grad)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["True\n","False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3E3E0C-Tgcsg"},"source":["# 4 - Empty gradients"]},{"cell_type":"markdown","metadata":{"id":"4tiiq_Nlgnmx"},"source":["Với backward() ta sẽ có đạo hàm tích lũy bên trong thuộc tính *.grad*. Chúng ta cần cẩn thận với nó trong quá trình tối ưu.\n","-> Sử dụng *.zero_()* cho đạo hàm trước khi bắt đầu bước tối ưu - điều này sẽ tránh lưu lại kết quả của lần đạo hàm trước đó. "]},{"cell_type":"code","metadata":{"id":"oTxRNXK6hDy2"},"source":["weights = torch.ones(4, requires_grad=True)\n","\n","for epoch in range(5):\n","  model_output = (weights*5).sum()\n","  model_output.backward()\n","\n","  print(weights.grad) \n","  # Tối ưu model bằng cách cập nhật trọng số sau khi đạo hàm\n","  with torch.no_grad():\n","    weights -= 0.001 * weights.grad\n","  \n","  weights.grad.zero_() # sử dụng empty gradients trước khi bắt đầu một lần tối ưu tiếp theo."],"execution_count":null,"outputs":[]}]}