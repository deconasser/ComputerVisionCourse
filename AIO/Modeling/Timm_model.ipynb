{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Fv-Q60OMPoCn"},"outputs":[],"source":["!git clone https://github.com/anminhhung/small_dog_cat_dataset"]},{"cell_type":"code","source":["!pip install torch==1.11.0+cu113 torchvision==0.12.0+cu113 torchaudio==0.11.0 --extra-index-url https://download.pytorch.org/whl/cu113"],"metadata":{"id":"CYEj6zS2p92l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install mmcv==1.3.9"],"metadata":{"id":"U9175xf60pvw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install git+https://github.com/rwightman/pytorch-image-models.git"],"metadata":{"id":"U0VInfQ6s2S3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git clone https://github.com/rwightman/pytorch-image-models"],"metadata":{"id":"ClPz2MqYtpWY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!mv small_dog_cat_dataset/ pytorch-image-models/"],"metadata":{"id":"U7hfNqTrt5DA","executionInfo":{"status":"ok","timestamp":1687705109653,"user_tz":-420,"elapsed":20,"user":{"displayName":"Minh H첫ng An","userId":"12219570561405750876"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["%cd pytorch-image-models/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5sONyD4huPfY","outputId":"64cae41e-5472-4635-b48a-32572917a78c","executionInfo":{"status":"ok","timestamp":1687705109655,"user_tz":-420,"elapsed":18,"user":{"displayName":"Minh H첫ng An","userId":"12219570561405750876"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/pytorch-image-models\n"]}]},{"cell_type":"markdown","source":["# Train"],"metadata":{"id":"AjNVjKmg2Mq2"}},{"cell_type":"code","source":["!export NCCL_DEBUG=INFO"],"metadata":{"id":"Us-FVH2w5tu-","executionInfo":{"status":"ok","timestamp":1687705131742,"user_tz":-420,"elapsed":390,"user":{"displayName":"Minh H첫ng An","userId":"12219570561405750876"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# EfficientNet-B2\n","!python train.py ./small_dog_cat_dataset/ --model efficientnet_b2 --pretrained --num-classes 2 --batch-size 2 --sched step --epochs 2 --decay-epochs 1 --decay-rate .9 --opt adamp --opt-eps .001 -j 8 --warmup-lr 1e-5 --weight-decay 1e-5 --lr 5e-5"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4uDn-kqruBT9","outputId":"731b4385-f7cd-4306-9bd7-2453c4ec9b29","executionInfo":{"status":"ok","timestamp":1687705591083,"user_tz":-420,"elapsed":422306,"user":{"displayName":"Minh H첫ng An","userId":"12219570561405750876"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Training with a single process on 1 device (cuda:0).\n","Loading pretrained weights from Hugging Face hub (timm/efficientnet_b2.ra_in1k)\n","[timm/efficientnet_b2.ra_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n","Model efficientnet_b2 created, param count:7703812\n","Data processing configuration for current model + dataset:\n","\tinput_size: (3, 256, 256)\n","\tinterpolation: bicubic\n","\tmean: (0.485, 0.456, 0.406)\n","\tstd: (0.229, 0.224, 0.225)\n","\tcrop_pct: 1.0\n","\tcrop_mode: center\n","AMP not enabled. Training in float32.\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:487: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Scheduled epochs: 2. LR stepped per epoch.\n","Train: 0 [   0/1000 (  0%)]  Loss: 0.499 (0.499)  Time: 1.493s,    1.34/s  (1.493s,    1.34/s)  LR: 1.000e-05  Data: 0.646 (0.646)\n","Train: 0 [  50/1000 (  5%)]  Loss: 3.47 (1.28)  Time: 0.218s,    9.17/s  (0.173s,   11.57/s)  LR: 1.000e-05  Data: 0.003 (0.015)\n","Train: 0 [ 100/1000 ( 10%)]  Loss: 0.683 (1.30)  Time: 0.145s,   13.84/s  (0.172s,   11.61/s)  LR: 1.000e-05  Data: 0.003 (0.009)\n","Train: 0 [ 150/1000 ( 15%)]  Loss: 0.336 (1.22)  Time: 0.218s,    9.19/s  (0.163s,   12.27/s)  LR: 1.000e-05  Data: 0.009 (0.007)\n","Train: 0 [ 200/1000 ( 20%)]  Loss: 1.95 (1.20)  Time: 0.142s,   14.10/s  (0.165s,   12.10/s)  LR: 1.000e-05  Data: 0.003 (0.006)\n","Train: 0 [ 250/1000 ( 25%)]  Loss: 0.316 (1.19)  Time: 0.214s,    9.33/s  (0.163s,   12.31/s)  LR: 1.000e-05  Data: 0.010 (0.006)\n","Train: 0 [ 300/1000 ( 30%)]  Loss: 1.87 (1.16)  Time: 0.139s,   14.42/s  (0.164s,   12.22/s)  LR: 1.000e-05  Data: 0.003 (0.005)\n","Train: 0 [ 350/1000 ( 35%)]  Loss: 2.51 (1.16)  Time: 0.589s,    3.39/s  (0.175s,   11.45/s)  LR: 1.000e-05  Data: 0.003 (0.005)\n","Train: 0 [ 400/1000 ( 40%)]  Loss: 1.47 (1.15)  Time: 0.135s,   14.78/s  (0.171s,   11.67/s)  LR: 1.000e-05  Data: 0.003 (0.005)\n","Train: 0 [ 450/1000 ( 45%)]  Loss: 0.425 (1.12)  Time: 0.139s,   14.41/s  (0.172s,   11.60/s)  LR: 1.000e-05  Data: 0.003 (0.005)\n","Train: 0 [ 500/1000 ( 50%)]  Loss: 0.229 (1.11)  Time: 0.136s,   14.76/s  (0.169s,   11.83/s)  LR: 1.000e-05  Data: 0.003 (0.005)\n","Train: 0 [ 550/1000 ( 55%)]  Loss: 1.70 (1.11)  Time: 0.144s,   13.91/s  (0.170s,   11.76/s)  LR: 1.000e-05  Data: 0.003 (0.005)\n","Train: 0 [ 600/1000 ( 60%)]  Loss: 1.55 (1.11)  Time: 0.137s,   14.56/s  (0.167s,   11.95/s)  LR: 1.000e-05  Data: 0.003 (0.005)\n","Train: 0 [ 650/1000 ( 65%)]  Loss: 0.207 (1.13)  Time: 0.137s,   14.60/s  (0.168s,   11.89/s)  LR: 1.000e-05  Data: 0.003 (0.005)\n","Train: 0 [ 700/1000 ( 70%)]  Loss: 1.43 (1.10)  Time: 0.145s,   13.75/s  (0.166s,   12.04/s)  LR: 1.000e-05  Data: 0.010 (0.005)\n","Train: 0 [ 750/1000 ( 75%)]  Loss: 1.13 (1.11)  Time: 0.139s,   14.36/s  (0.167s,   11.98/s)  LR: 1.000e-05  Data: 0.003 (0.005)\n","Train: 0 [ 800/1000 ( 80%)]  Loss: 0.229 (1.09)  Time: 0.140s,   14.30/s  (0.165s,   12.11/s)  LR: 1.000e-05  Data: 0.003 (0.005)\n","Train: 0 [ 850/1000 ( 85%)]  Loss: 0.206 (1.07)  Time: 0.135s,   14.79/s  (0.166s,   12.06/s)  LR: 1.000e-05  Data: 0.003 (0.005)\n","Train: 0 [ 900/1000 ( 90%)]  Loss: 1.36 (1.07)  Time: 0.135s,   14.80/s  (0.165s,   12.15/s)  LR: 1.000e-05  Data: 0.003 (0.005)\n","Train: 0 [ 950/1000 ( 95%)]  Loss: 0.827 (1.06)  Time: 0.134s,   14.93/s  (0.165s,   12.10/s)  LR: 1.000e-05  Data: 0.002 (0.005)\n","Test: [   0/1299]  Time: 0.689 (0.689)  Loss:   0.005 ( 0.005)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)\n","Test: [  50/1299]  Time: 0.027 (0.039)  Loss:   0.809 ( 1.042)  Acc@1:  50.000 ( 70.588)  Acc@5: 100.000 (100.000)\n","Test: [ 100/1299]  Time: 0.061 (0.042)  Loss:   2.023 ( 1.011)  Acc@1:   0.000 ( 69.307)  Acc@5: 100.000 (100.000)\n","Test: [ 150/1299]  Time: 0.055 (0.043)  Loss:   1.761 ( 1.026)  Acc@1:  50.000 ( 67.219)  Acc@5: 100.000 (100.000)\n","Test: [ 200/1299]  Time: 0.029 (0.043)  Loss:   2.558 ( 0.967)  Acc@1:   0.000 ( 68.905)  Acc@5: 100.000 (100.000)\n","Test: [ 250/1299]  Time: 0.035 (0.041)  Loss:   0.080 ( 0.897)  Acc@1: 100.000 ( 70.916)  Acc@5: 100.000 (100.000)\n","Test: [ 300/1299]  Time: 0.025 (0.038)  Loss:   0.179 ( 0.915)  Acc@1: 100.000 ( 71.262)  Acc@5: 100.000 (100.000)\n","Test: [ 350/1299]  Time: 0.024 (0.036)  Loss:   2.312 ( 0.946)  Acc@1:   0.000 ( 70.228)  Acc@5: 100.000 (100.000)\n","Test: [ 400/1299]  Time: 0.026 (0.035)  Loss:   0.425 ( 0.934)  Acc@1: 100.000 ( 69.327)  Acc@5: 100.000 (100.000)\n","Test: [ 450/1299]  Time: 0.025 (0.034)  Loss:   0.490 ( 0.937)  Acc@1:  50.000 ( 69.401)  Acc@5: 100.000 (100.000)\n","Test: [ 500/1299]  Time: 0.025 (0.033)  Loss:   4.362 ( 0.937)  Acc@1:  50.000 ( 69.760)  Acc@5: 100.000 (100.000)\n","Test: [ 550/1299]  Time: 0.027 (0.032)  Loss:   0.894 ( 0.967)  Acc@1:  50.000 ( 69.601)  Acc@5: 100.000 (100.000)\n","Test: [ 600/1299]  Time: 0.025 (0.032)  Loss:   0.127 ( 0.978)  Acc@1: 100.000 ( 69.218)  Acc@5: 100.000 (100.000)\n","Test: [ 650/1299]  Time: 0.045 (0.033)  Loss:   1.139 ( 0.981)  Acc@1:  50.000 ( 69.124)  Acc@5: 100.000 (100.000)\n","Test: [ 700/1299]  Time: 0.031 (0.033)  Loss:   3.555 ( 1.009)  Acc@1:  50.000 ( 68.759)  Acc@5: 100.000 (100.000)\n","Test: [ 750/1299]  Time: 0.031 (0.034)  Loss:   1.331 ( 1.011)  Acc@1:   0.000 ( 68.509)  Acc@5: 100.000 (100.000)\n","Test: [ 800/1299]  Time: 0.026 (0.034)  Loss:   0.218 ( 1.029)  Acc@1: 100.000 ( 68.477)  Acc@5: 100.000 (100.000)\n","Test: [ 850/1299]  Time: 0.024 (0.034)  Loss:   0.059 ( 1.010)  Acc@1: 100.000 ( 68.919)  Acc@5: 100.000 (100.000)\n","Test: [ 900/1299]  Time: 0.026 (0.033)  Loss:   0.074 ( 1.000)  Acc@1: 100.000 ( 69.312)  Acc@5: 100.000 (100.000)\n","Test: [ 950/1299]  Time: 0.024 (0.033)  Loss:   1.126 ( 0.979)  Acc@1:  50.000 ( 69.821)  Acc@5: 100.000 (100.000)\n","Test: [1000/1299]  Time: 0.026 (0.033)  Loss:   0.011 ( 0.961)  Acc@1: 100.000 ( 70.280)  Acc@5: 100.000 (100.000)\n","Test: [1050/1299]  Time: 0.029 (0.032)  Loss:   0.003 ( 0.946)  Acc@1: 100.000 ( 70.599)  Acc@5: 100.000 (100.000)\n","Test: [1100/1299]  Time: 0.027 (0.032)  Loss:   0.354 ( 0.916)  Acc@1: 100.000 ( 71.435)  Acc@5: 100.000 (100.000)\n","Test: [1150/1299]  Time: 0.044 (0.032)  Loss:   0.051 ( 0.906)  Acc@1: 100.000 ( 71.764)  Acc@5: 100.000 (100.000)\n","Test: [1200/1299]  Time: 0.059 (0.032)  Loss:   0.464 ( 0.885)  Acc@1:  50.000 ( 72.523)  Acc@5: 100.000 (100.000)\n","Test: [1250/1299]  Time: 0.036 (0.033)  Loss:   3.152 ( 0.876)  Acc@1:  50.000 ( 72.902)  Acc@5: 100.000 (100.000)\n","Test: [1299/1299]  Time: 0.019 (0.033)  Loss:   0.396 ( 0.861)  Acc@1:  50.000 ( 73.231)  Acc@5: 100.000 (100.000)\n","Current checkpoints:\n"," ('./output/train/20230625-145935-efficientnet_b2-256/checkpoint-0.pth.tar', 73.23076923076923)\n","\n","Train: 1 [   0/1000 (  0%)]  Loss: 1.33 (1.33)  Time: 0.359s,    5.58/s  (0.359s,    5.58/s)  LR: 1.800e-05  Data: 0.147 (0.147)\n","Train: 1 [  50/1000 (  5%)]  Loss: 0.562 (1.04)  Time: 0.134s,   14.97/s  (0.144s,   13.91/s)  LR: 1.800e-05  Data: 0.002 (0.006)\n","Train: 1 [ 100/1000 ( 10%)]  Loss: 0.228 (0.951)  Time: 0.131s,   15.23/s  (0.163s,   12.26/s)  LR: 1.800e-05  Data: 0.002 (0.006)\n","Train: 1 [ 150/1000 ( 15%)]  Loss: 2.50 (0.989)  Time: 0.138s,   14.54/s  (0.155s,   12.93/s)  LR: 1.800e-05  Data: 0.003 (0.005)\n","Train: 1 [ 200/1000 ( 20%)]  Loss: 3.25 (0.981)  Time: 0.136s,   14.75/s  (0.162s,   12.37/s)  LR: 1.800e-05  Data: 0.002 (0.005)\n","Train: 1 [ 250/1000 ( 25%)]  Loss: 1.70 (0.952)  Time: 0.136s,   14.70/s  (0.157s,   12.75/s)  LR: 1.800e-05  Data: 0.003 (0.005)\n","Train: 1 [ 300/1000 ( 30%)]  Loss: 1.12 (0.943)  Time: 0.146s,   13.70/s  (0.161s,   12.40/s)  LR: 1.800e-05  Data: 0.002 (0.005)\n","Train: 1 [ 350/1000 ( 35%)]  Loss: 0.258 (0.964)  Time: 0.137s,   14.58/s  (0.158s,   12.67/s)  LR: 1.800e-05  Data: 0.003 (0.005)\n","Train: 1 [ 400/1000 ( 40%)]  Loss: 0.241 (0.956)  Time: 0.135s,   14.77/s  (0.161s,   12.43/s)  LR: 1.800e-05  Data: 0.002 (0.005)\n","Train: 1 [ 450/1000 ( 45%)]  Loss: 0.663 (0.953)  Time: 0.132s,   15.11/s  (0.158s,   12.64/s)  LR: 1.800e-05  Data: 0.003 (0.005)\n","Train: 1 [ 500/1000 ( 50%)]  Loss: 0.494 (0.929)  Time: 0.231s,    8.67/s  (0.161s,   12.46/s)  LR: 1.800e-05  Data: 0.012 (0.005)\n","Train: 1 [ 550/1000 ( 55%)]  Loss: 0.368 (0.912)  Time: 0.139s,   14.39/s  (0.159s,   12.61/s)  LR: 1.800e-05  Data: 0.005 (0.005)\n","Train: 1 [ 600/1000 ( 60%)]  Loss: 0.760 (0.897)  Time: 0.229s,    8.73/s  (0.160s,   12.49/s)  LR: 1.800e-05  Data: 0.009 (0.005)\n","Train: 1 [ 650/1000 ( 65%)]  Loss: 0.894 (0.888)  Time: 0.132s,   15.18/s  (0.158s,   12.63/s)  LR: 1.800e-05  Data: 0.002 (0.005)\n","Train: 1 [ 700/1000 ( 70%)]  Loss: 1.80 (0.880)  Time: 0.202s,    9.91/s  (0.159s,   12.55/s)  LR: 1.800e-05  Data: 0.003 (0.005)\n","Train: 1 [ 750/1000 ( 75%)]  Loss: 1.33 (0.883)  Time: 0.133s,   15.03/s  (0.158s,   12.63/s)  LR: 1.800e-05  Data: 0.003 (0.005)\n","Train: 1 [ 800/1000 ( 80%)]  Loss: 0.368 (0.876)  Time: 0.235s,    8.52/s  (0.159s,   12.58/s)  LR: 1.800e-05  Data: 0.010 (0.005)\n","Train: 1 [ 850/1000 ( 85%)]  Loss: 0.279 (0.867)  Time: 0.135s,   14.77/s  (0.159s,   12.61/s)  LR: 1.800e-05  Data: 0.002 (0.005)\n","Train: 1 [ 900/1000 ( 90%)]  Loss: 0.284 (0.862)  Time: 0.221s,    9.05/s  (0.159s,   12.56/s)  LR: 1.800e-05  Data: 0.016 (0.005)\n","Train: 1 [ 950/1000 ( 95%)]  Loss: 0.309 (0.857)  Time: 0.165s,   12.11/s  (0.159s,   12.59/s)  LR: 1.800e-05  Data: 0.003 (0.005)\n","Test: [   0/1299]  Time: 0.262 (0.262)  Loss:   0.001 ( 0.001)  Acc@1: 100.000 (100.000)  Acc@5: 100.000 (100.000)\n","Test: [  50/1299]  Time: 0.026 (0.047)  Loss:   0.217 ( 0.687)  Acc@1: 100.000 ( 78.431)  Acc@5: 100.000 (100.000)\n","Test: [ 100/1299]  Time: 0.027 (0.037)  Loss:   0.846 ( 0.656)  Acc@1:   0.000 ( 77.723)  Acc@5: 100.000 (100.000)\n","Test: [ 150/1299]  Time: 0.026 (0.033)  Loss:   2.287 ( 0.638)  Acc@1:  50.000 ( 78.146)  Acc@5: 100.000 (100.000)\n","Test: [ 200/1299]  Time: 0.026 (0.032)  Loss:   2.000 ( 0.700)  Acc@1:   0.000 ( 76.368)  Acc@5: 100.000 (100.000)\n","Test: [ 250/1299]  Time: 0.027 (0.031)  Loss:   0.083 ( 0.721)  Acc@1: 100.000 ( 76.494)  Acc@5: 100.000 (100.000)\n","Test: [ 300/1299]  Time: 0.029 (0.030)  Loss:   0.316 ( 0.767)  Acc@1: 100.000 ( 75.581)  Acc@5: 100.000 (100.000)\n","Test: [ 350/1299]  Time: 0.024 (0.030)  Loss:   3.636 ( 0.765)  Acc@1:  50.000 ( 75.641)  Acc@5: 100.000 (100.000)\n","Test: [ 400/1299]  Time: 0.027 (0.029)  Loss:   0.286 ( 0.738)  Acc@1: 100.000 ( 75.935)  Acc@5: 100.000 (100.000)\n","Test: [ 450/1299]  Time: 0.048 (0.030)  Loss:   1.211 ( 0.724)  Acc@1:  50.000 ( 75.721)  Acc@5: 100.000 (100.000)\n","Test: [ 500/1299]  Time: 0.053 (0.031)  Loss:   1.582 ( 0.705)  Acc@1:  50.000 ( 76.148)  Acc@5: 100.000 (100.000)\n","Test: [ 550/1299]  Time: 0.046 (0.032)  Loss:   0.610 ( 0.714)  Acc@1:  50.000 ( 75.499)  Acc@5: 100.000 (100.000)\n","Test: [ 600/1299]  Time: 0.026 (0.033)  Loss:   0.213 ( 0.714)  Acc@1: 100.000 ( 75.208)  Acc@5: 100.000 (100.000)\n","Test: [ 650/1299]  Time: 0.026 (0.033)  Loss:   0.665 ( 0.707)  Acc@1:  50.000 ( 75.499)  Acc@5: 100.000 (100.000)\n","Test: [ 700/1299]  Time: 0.026 (0.032)  Loss:   0.877 ( 0.706)  Acc@1:  50.000 ( 75.963)  Acc@5: 100.000 (100.000)\n","Test: [ 750/1299]  Time: 0.025 (0.032)  Loss:   0.276 ( 0.702)  Acc@1: 100.000 ( 75.899)  Acc@5: 100.000 (100.000)\n","Test: [ 800/1299]  Time: 0.025 (0.031)  Loss:   1.587 ( 0.694)  Acc@1:   0.000 ( 75.968)  Acc@5: 100.000 (100.000)\n","Test: [ 850/1299]  Time: 0.025 (0.031)  Loss:   0.133 ( 0.697)  Acc@1: 100.000 ( 75.558)  Acc@5: 100.000 (100.000)\n","Test: [ 900/1299]  Time: 0.026 (0.031)  Loss:   0.227 ( 0.706)  Acc@1: 100.000 ( 75.361)  Acc@5: 100.000 (100.000)\n","Test: [ 950/1299]  Time: 0.024 (0.030)  Loss:   0.113 ( 0.711)  Acc@1: 100.000 ( 75.289)  Acc@5: 100.000 (100.000)\n","Test: [1000/1299]  Time: 0.055 (0.031)  Loss:   0.009 ( 0.713)  Acc@1: 100.000 ( 75.375)  Acc@5: 100.000 (100.000)\n","Test: [1050/1299]  Time: 0.025 (0.031)  Loss:   0.001 ( 0.716)  Acc@1: 100.000 ( 75.595)  Acc@5: 100.000 (100.000)\n","Test: [1100/1299]  Time: 0.064 (0.033)  Loss:   1.268 ( 0.706)  Acc@1:  50.000 ( 75.704)  Acc@5: 100.000 (100.000)\n","Test: [1150/1299]  Time: 0.091 (0.034)  Loss:   0.230 ( 0.709)  Acc@1: 100.000 ( 75.717)  Acc@5: 100.000 (100.000)\n","Test: [1200/1299]  Time: 0.027 (0.035)  Loss:   0.511 ( 0.703)  Acc@1:  50.000 ( 75.729)  Acc@5: 100.000 (100.000)\n","Test: [1250/1299]  Time: 0.024 (0.035)  Loss:   1.494 ( 0.700)  Acc@1:  50.000 ( 75.739)  Acc@5: 100.000 (100.000)\n","Test: [1299/1299]  Time: 0.017 (0.034)  Loss:   0.218 ( 0.694)  Acc@1: 100.000 ( 76.115)  Acc@5: 100.000 (100.000)\n","Current checkpoints:\n"," ('./output/train/20230625-145935-efficientnet_b2-256/checkpoint-1.pth.tar', 76.11538461538461)\n"," ('./output/train/20230625-145935-efficientnet_b2-256/checkpoint-0.pth.tar', 73.23076923076923)\n","\n","*** Best metric: 76.11538461538461 (epoch 1)\n"]}]},{"cell_type":"markdown","source":["# Searching for models"],"metadata":{"id":"VAY3x6n3FrKr"}},{"cell_type":"code","source":["import timm\n","\n","timm.list_models('*efficientnet*')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lEbPuj17FwXm","executionInfo":{"status":"ok","timestamp":1687705602065,"user_tz":-420,"elapsed":1523,"user":{"displayName":"Minh H첫ng An","userId":"12219570561405750876"}},"outputId":"83720b1a-b92f-4f61-801b-c6ffeeaafab1"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['efficientnet_b0',\n"," 'efficientnet_b0_g8_gn',\n"," 'efficientnet_b0_g16_evos',\n"," 'efficientnet_b0_gn',\n"," 'efficientnet_b1',\n"," 'efficientnet_b1_pruned',\n"," 'efficientnet_b2',\n"," 'efficientnet_b2_pruned',\n"," 'efficientnet_b2a',\n"," 'efficientnet_b3',\n"," 'efficientnet_b3_g8_gn',\n"," 'efficientnet_b3_gn',\n"," 'efficientnet_b3_pruned',\n"," 'efficientnet_b3a',\n"," 'efficientnet_b4',\n"," 'efficientnet_b5',\n"," 'efficientnet_b6',\n"," 'efficientnet_b7',\n"," 'efficientnet_b8',\n"," 'efficientnet_cc_b0_4e',\n"," 'efficientnet_cc_b0_8e',\n"," 'efficientnet_cc_b1_8e',\n"," 'efficientnet_el',\n"," 'efficientnet_el_pruned',\n"," 'efficientnet_em',\n"," 'efficientnet_es',\n"," 'efficientnet_es_pruned',\n"," 'efficientnet_l2',\n"," 'efficientnet_lite0',\n"," 'efficientnet_lite1',\n"," 'efficientnet_lite2',\n"," 'efficientnet_lite3',\n"," 'efficientnet_lite4',\n"," 'efficientnetv2_l',\n"," 'efficientnetv2_m',\n"," 'efficientnetv2_rw_m',\n"," 'efficientnetv2_rw_s',\n"," 'efficientnetv2_rw_t',\n"," 'efficientnetv2_s',\n"," 'efficientnetv2_xl',\n"," 'gc_efficientnetv2_rw_t',\n"," 'tf_efficientnet_b0',\n"," 'tf_efficientnet_b1',\n"," 'tf_efficientnet_b2',\n"," 'tf_efficientnet_b3',\n"," 'tf_efficientnet_b4',\n"," 'tf_efficientnet_b5',\n"," 'tf_efficientnet_b6',\n"," 'tf_efficientnet_b7',\n"," 'tf_efficientnet_b8',\n"," 'tf_efficientnet_cc_b0_4e',\n"," 'tf_efficientnet_cc_b0_8e',\n"," 'tf_efficientnet_cc_b1_8e',\n"," 'tf_efficientnet_el',\n"," 'tf_efficientnet_em',\n"," 'tf_efficientnet_es',\n"," 'tf_efficientnet_l2',\n"," 'tf_efficientnet_lite0',\n"," 'tf_efficientnet_lite1',\n"," 'tf_efficientnet_lite2',\n"," 'tf_efficientnet_lite3',\n"," 'tf_efficientnet_lite4',\n"," 'tf_efficientnetv2_b0',\n"," 'tf_efficientnetv2_b1',\n"," 'tf_efficientnetv2_b2',\n"," 'tf_efficientnetv2_b3',\n"," 'tf_efficientnetv2_l',\n"," 'tf_efficientnetv2_m',\n"," 'tf_efficientnetv2_s',\n"," 'tf_efficientnetv2_xl']"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["timm.list_models('*b3a')[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DGo3WHeeF3xt","executionInfo":{"status":"ok","timestamp":1687705602067,"user_tz":-420,"elapsed":21,"user":{"displayName":"Minh H첫ng An","userId":"12219570561405750876"}},"outputId":"d874a397-62a6-4aef-efca-7584195dcfc2"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['efficientnet_b3a']"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["timm.list_models('resne*t*', pretrained=True)[:10]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l1yIGHe_F8dy","executionInfo":{"status":"ok","timestamp":1687705606669,"user_tz":-420,"elapsed":10,"user":{"displayName":"Minh H첫ng An","userId":"12219570561405750876"}},"outputId":"b8655824-94da-4dda-99b9-69708627e10d"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['resnest14d.gluon_in1k',\n"," 'resnest26d.gluon_in1k',\n"," 'resnest50d.in1k',\n"," 'resnest50d_1s4x24d.in1k',\n"," 'resnest50d_4s2x40d.in1k',\n"," 'resnest101e.in1k',\n"," 'resnest200e.in1k',\n"," 'resnest269e.in1k',\n"," 'resnet10t.c3_in1k',\n"," 'resnet14t.c3_in1k']"]},"metadata":{},"execution_count":14}]}]}